{"aws-ec2-tf-cf":{"title":"Deploying EC2 with... everything.","links":[],"tags":["aws","cloud","VMs","EC2","CloudFront","Terraform","OpenTofu","IaC","CDK"],"content":"Let’s get straight to business.\nWhat do we want to do?\nDeploy an AWS EC2 instance in a fantastically diverse collection of ways. Not including clicking through things in a web UI (that’s cool but too slow and too easy, thus very boring) nor especially pulling an untrustworthy LLM-generated YAML or Bash script from Amazon Q itself (or loose sequence of 3 commands, to be more precise).\nWhich download we sure did!\nSo, now, let’s break that nightmare down.\n\n\n                  \n                  The LLM (Ludicrous Laziness Methodology) Zone \n                  \n                \n\n\n\n                  \n                  Note \n                  \n                \n\nObviously, sensitive information about actual things I’m deploying and my IP address have been redacted. Not that I’m worried, anyways - my ISP locked me behind CG-NAT with no IPv6, after all *ugly sobbing*.\n\n\nBefore we begin, let’s make one thing clear - I already have set up a VPC, a SSH keyset and a security group. Then I went to create a very specific kind of EC2 instance. AWS gave me two options for exporting my EC2 params: an “AI generated YAML”, or a sequence of long aws commands (probably also with a neural network shoved in there somewhere for no practical reason whatsoever):\nResources:\n EC2SecurityGroup:\n   Type: AWS::EC2::SecurityGroup\n   Properties:\n     GroupName: launch-wizard-1\n     GroupDescription: launch-wizard-1 created 2020-08-28T00:57:57.123Z\n     VpcId: vpc-0e2cd7c927ac9484e\n     SecurityGroupIngress:\n       - IpProtocol: tcp\n         FromPort: 22\n         ToPort: 22\n         CidrIp: 80.0.81.35/32\n \n EC2Instance:\n   Type: AWS::EC2::Instance\n   Properties:\n     ImageId: ami-07a4c6232c4e5a0ec\n     InstanceType: t4g.nano\n     BlockDeviceMappings:\n       - DeviceName: /dev/xvda\n         Ebs:\n           Encrypted: false\n           DeleteOnTermination: true\n           Iops: 3000\n           SnapshotId: snap-0f815581f5eda1e29\n           VolumeSize: 8\n           VolumeType: gp3\n           Throughput: 125\n     NetworkInterfaces:\n       - AssociatePublicIpAddress: true\n         DeviceIndex: 0\n         GroupSet: \n           - !Ref EC2SecurityGroup\n     CreditSpecification:\n       CPUCredits: unlimited\n     Tags:\n       - Key: Name\n         Value: My Badass Server\n       - Key: Codename  \n         Value: my-badass-serevr\n       - Key: Type\n         Value: BastionServer\n     MetadataOptions:\n       HttpEndpoint: enabled\n       HttpPutResponseHopLimit: 2\n       HttpTokens: required\n     PrivateDnsNameOptions:\n       HostnameType: ip-name\n       EnableResourceNameDnsARecord: true\n       EnableResourceNameDnsAAAARecord: false\nAnd the corresponding Bash hellcommand sequence:\n#!/usr/bin/bash\naws ec2 create-security-group --group-name &quot;launch-wizard-1&quot; --description &quot;launch-wizard-1 created 2020-08-28T00:57:57.123Z&quot; --vpc-id &quot;vpc-0e2cd7c927ac9484e&quot; \n \naws ec2 authorize-security-group-ingress --group-id &quot;sg-preview-1&quot; --ip-permissions &#039;{&quot;IpProtocol&quot;:&quot;tcp&quot;,&quot;FromPort&quot;:22,&quot;ToPort&quot;:22,&quot;IpRanges&quot;:[{&quot;CidrIp&quot;:&quot;80.0.81.35/32&quot;}]}&#039; \n \naws ec2 run-instances --image-id &quot;ami-07a4c6232c4e5a0ec&quot; --instance-type &quot;t4g.nano&quot; --block-device-mappings &#039;{&quot;DeviceName&quot;:&quot;/dev/xvda&quot;,&quot;Ebs&quot;:{&quot;Encrypted&quot;:false,&quot;DeleteOnTermination&quot;:true,&quot;Iops&quot;:3000,&quot;SnapshotId&quot;:&quot;snap-0f815581f5eda1e29&quot;,&quot;VolumeSize&quot;:8,&quot;VolumeType&quot;:&quot;gp3&quot;,&quot;Throughput&quot;:125}}&#039; --network-interfaces &#039;{&quot;AssociatePublicIpAddress&quot;:true,&quot;DeviceIndex&quot;:0,&quot;Groups&quot;:[&quot;sg-preview-1&quot;]}&#039; --credit-specification &#039;{&quot;CpuCredits&quot;:&quot;unlimited&quot;}&#039; --tag-specifications &#039;{&quot;ResourceType&quot;:&quot;instance&quot;,&quot;Tags&quot;:[{&quot;Key&quot;:&quot;Name&quot;,&quot;Value&quot;:&quot;My Badass Server&quot;},{&quot;Key&quot;:&quot;Codename&quot;,&quot;Value&quot;:&quot;my-badass-server&quot;},{&quot;Key&quot;:&quot;Type&quot;,&quot;Value&quot;:&quot;BastionServer&quot;}]}&#039; --metadata-options &#039;{&quot;HttpEndpoint&quot;:&quot;enabled&quot;,&quot;HttpPutResponseHopLimit&quot;:2,&quot;HttpTokens&quot;:&quot;required&quot;}&#039; --private-dns-name-options &#039;{&quot;HostnameType&quot;:&quot;ip-name&quot;,&quot;EnableResourceNameDnsARecord&quot;:true,&quot;EnableResourceNameDnsAAAARecord&quot;:false}&#039; --count &quot;1&quot; \nHellish indeed. Surely I can do it better just by reading a bit of documentation, right?\n\n\nWhat?\nI like the feeling of knowing what I’m doing, so let’s dive into some prerequisites.\nWhat can we do?\nTerraform\nOpenTofu\nCloudFront"},"bitflips":{"title":"Bitflips","links":["linux-kernel"],"tags":["bitflip","RAM","memory","errors","btrfs","unix","linux","kernel","osdev"],"content":"Bitflips\nBitflip is a kind of memory error that is an unintentional state switch from 0 to 1 or vice versa. Typically it affects random access memory.\nDiagnosing bitflips\nIn this Btrfs Linux Kernel Mailing List 2025-01 thread, an Arch Linux user reported issues with their filesystem, sharing dmesg logs of Btrfs errors regarding leaf corruption, resulting in their filesystem going read-only:\n[  +0.000001] BTRFS critical (device nvme0n1p2): corrupt leaf: block=3279774253056 slot=66 extent bytenr=3148007481344 len=8192 invalid extent refs, have 1 expect &gt;= inline 513\n[  +0.000005] BTRFS info (device nvme0n1p2): leaf 3279774253056 gen 381142 total ptrs 198 free space 1189 owner 2\n...\n[  +0.000002] BTRFS error (device nvme0n1p2): block=3279774253056 write time tree block corruption detected\n[  +0.007065] BTRFS: error (device nvme0n1p2) in btrfs_commit_transaction:2523: errno=-5 IO failure (Error while writing out transaction)\n[  +0.000004] BTRFS info (device nvme0n1p2 state E): forced readonly\nOne of Btrfs maintainers Qu Wenruo quickly identified it as a faulty hardware issue by looking for obvious bit flips.\nConsider the following dmesg log excerpt:\n[  +0.000001] \titem 66 key (3148007481344 168 8192) itemoff 13022 itemsize 53\n[  +0.000001] \t\textent refs 1 gen 380990 flags 1\n[  +0.000001] \t\tref#0: extent data backref root 260 objectid 68965 offset 407224320 count 513\nQuoting Qu:\n\nThis is the offending bad extent item.\nFirstly it shows the extent item should have only 1 ref (“extent refs 1”).\nBut the inlined one has ref count 513, completely beyond the expected 1 ref.\nhex(513) = 0x201\nhex(1)   = 0x001\nVery obvious bitflip.\n"},"btrfs-internals":{"title":"Btrfs internals","links":[],"tags":["btrfs","filesystems","linux","C"],"content":"Btrfs internals"},"btrfs":{"title":"Btrfs","links":[],"tags":["btrfs","filesystems","linux"],"content":"Btrfs"},"css":{"title":"CSS","links":[],"tags":["webdev","css","html","art"],"content":"CSS (Cascading Style Sheets)\nCSS reset - a method of resetting default styling of some HTML tag, used by most browsers. It can be done, for example, like this:\nhtml, body, div, span, applet, object, iframe,\nh1, h2, h3, h4, h5, h6, p, blockquote, pre,\na, abbr, acronym, address, big, cite, code,\ndel, dfn, em, img, ins, kbd, q, s, samp,\nsmall, strike, strong, sub, sup, tt, var,\nb, u, i, center,\ndl, dt, dd, ol, ul, li,\nfieldset, form, label, legend,\ntable, caption, tbody, tfoot, thead, tr, th, td,\narticle, aside, canvas, details, embed, \nfigure, figcaption, footer, header, hgroup, \nmenu, nav, output, ruby, section, summary,\ntime, mark, audio, video {\n\tmargin: 0;\n\tpadding: 0;\n\tborder: 0;\n\tfont-size: 100%;\n\tfont: inherit;\n\tvertical-align: baseline;\n}"},"curl-cheatsheet":{"title":"cURL cheatsheet","links":[],"tags":["curl","networking","term"],"content":"cURL cheatsheet\nUse a custom loopback interface (e.g. 127.1.2.3):\ncurl --interface 127.1.2.3 -v http://127.57.57.57:8057/\n\nIndex of helpful links\nEverything curl"},"docker":{"title":"Docker basics","links":["dockerfiles"],"tags":["Docker","Dockerfile","containers","term","Go"],"content":"Docker\nDocker Engine\nIt runs the whole thing.\nDocker Build\nBuild images from Dockerfiles. In terms of command line toys, there’s docker build and docker buildx build which is like build on rails.\nDocker Buildx though is the client interface for building &amp; managing images, whereas Docker BuildKit is the server that does the heavy lifting including execution.\nBuilx’s build request to BuildKit includes the Dockerfile, build args and export and/or caching opts.\nExample Dockerfile\nThere’s a separate note about Dockerfiles.\nFrom github.com/cloudnativedevops/demo.git.\nAnyways, here’s a very plain and simple Go HTTP server that runs on 127.0.0.1:8888:\npackage main\n \nimport (\n\t&quot;fmt&quot;\n\t&quot;log&quot;\n\t&quot;net/http&quot;\n)\n \nfunc handler(w http.ResponseWriter, r *http.Request) {\n\tfmt.Fprintln(w, &quot;Hello, 世界&quot;)\n}\n \nfunc main() {\n\thttp.HandleFunc(&quot;/&quot;, handler)\n\tfmt.Println(&quot;Running demo app. Press Ctrl+C to exit...&quot;)\n\tlog.Fatal(http.ListenAndServe(&quot;:8888&quot;, nil))\n}\nNow here’s the Dockerfile\nFROM golang:1.17-alpine AS build\n \nWORKDIR /src/\nCOPY main.go go.* /src/\nRUN CGO_ENABLED=0 go build -o /bin/demo\n \nFROM scratch\nCOPY --from=build /bin/demo /bin/demo\nENTRYPOINT [&quot;/bin/demo&quot;]\nThen you run one of those1 to build and run at 127.0.0.1:9999:\ndocker image build -t myhello .\ndocker container run -p 9999:8888 myhello\n\n\n                  \n                  Caution \n                  \n                \n\nThe port syntax for docker run is HOST_PORT:CONTAINER_PORT, so to access the container and receive the pretty “Hello, 世界” you refer to your host’s address with the first port you provided.\ncurl -v 127.0.0.1:9999\n\n\nInspect your image &amp; container (prints a JSON to stdout):\ndocker image inspect myhello\ndocker inspect myhello\ndocker container inspect c7112281a566\ndocker inspect c7112281a566\nAlso check if it’s running:\ndocker ps\ndocker container ls\n\n\n                  \n                  Container names \n                  \n                \n\nWhen you build an image, by default it just gets a hexadecimal ID, which you can use to refer to it later (for example, to run it). These IDs aren’t particularly memorable or easy to type, so Docker allows you to give the image a human-readable name, using the -t switch to docker image build. In the previous example you named the image myhello, so you should be able to use that name to run the image now.\n(Credit: writers of book from parent chapter’s repository)\n\n\nDocker Compose\nDocker, but on rails (featuring YAML from DevOps May Cry).\nFootnotes\n\n\nOf course, appending sudo to both commands in the case I’m still a silly GoobSec shellhead that doesn’t wanna create a Docker group. ↩\n\n\n"},"dockerfiles":{"title":"Dockerfile structure","links":["docker"],"tags":["Dockerfile","Docker","containers","term"],"content":"Dockerfiles\nFor little extras, also read up Docker basics"},"find":{"title":"find","links":[],"tags":["find","linux"],"content":"find\nfind is a UNIX command for file search. It has many extra toy features that let you execute various other commands on files returned on search.\nAssorted usage examples\nFind all files in current working directory such that:\n\nAll filepaths matching the “*python3.12” expression are pruned (ignored)\nThe righthand expression of “-o” AKA “-or” is evaluated, that is:\nOnly search for files with .py extension.\nIn other words, find me all Python files without the environment. ‘bit stupid, I could just do find . -I &quot;python3.12&quot; -name &quot;*.py&quot; to ignore the env folder, but you never know + it’s still a helpful construct to know.\n\nfind . -path &quot;*python3.12&quot; -o -name &quot;*.py&quot;\nGrep through all code files\nYou can chain -exec’s with the help of \\; expression.\n-print0 prints the full file name to stdout followed by a NULL character instead of a newline.\nfind . -name &quot;*.java&quot; -print0 -exec echo {} \\; -exec grep url {} \\;\n\nResolve the full path of found files (and optionally nuke them)\nfind . -name &quot;index.html&quot; -exec realpath {} +\nfind . -name &quot;index.html&quot; -exec realpath {} \\; -exec rm {} \\;\n\n\n                  \n                  Hint\n                  \n                \n\nWhen trying to just delete resolved files, literally just -exec rm and that’ll suffice:\nfind . -name &quot;TaskEvent.java&quot; -exec rm {} +\n\n\nDelete all matched files, but search no further than ‘level 1 subdirectories’\nfind . -maxdepth 1 -name &quot;*bash_history*&quot; -exec rm {} +\n\n\n                  \n                  Help\n                  \n                \n\nExtra info on manipulating with search depths from find manual:\n -maxdepth levels\n        Descend at most levels (a non-negative integer) levels of  directories  below\n        the  starting-points.   Using  -maxdepth 0 means only apply the tests and ac‐\n        tions to the starting-points themselves.\n\n -mindepth levels\n        Do not apply any tests or actions at levels less than levels (a  non-negative\n        integer).   Using  -mindepth 1  means  process all files except the starting-\n        points.\n\n\n"},"firewalls":{"title":"Firewalls on Linux","links":[],"tags":["firewalls","firewalld","ufw","linux","term"],"content":"firewalld\nFirewall.\nufw\nAKA uncomplicated firewall; a firewall config tool running on top of iptables. Common with Ubuntu-based distros (e.g. Linux Mint).\nReloading ufw:\nsudo ufw reload"},"git":{"title":"Git","links":[],"tags":["git","vcs","term"],"content":"Git\nCommand cheatsheet\n\nRemoving untracked files from the working tree\ngit rm -rf --cached . (purge from tracking index if already staged)\ngit clean -dfX\nDry run:\ngit clean -dnX\n"},"github":{"title":"GitHub","links":[],"tags":["GitHub","Git","repositories","term"],"content":"GitHub CLI reference\nGitHub web\n\n\n                  \n                  Remote repo editing \n                  \n                \n\nOpen a repository in browser, then click on a file and press the &lt;/&gt; key to open the github.dev VS Code browser editor!\n\n"},"grafana":{"title":"Grafana","links":["prometheus"],"tags":["sre","monitoring","grafana"],"content":"Grafana\nGrafana creating monitoring dashboards based on data from endpoints provided to it. It’s like a “frontend” for monitoring data scraper tools such as Prometheus, DataDog, CloudWatch and so on."},"index":{"title":"Milky Quartz on the catnet","links":[],"tags":[],"content":"Welcome to milky-quartz.netcatcat.net ‼️\nMilky Quartz! On the cat net catnet, on the net!\nThis subdomain is a big garden for all my notes surroundings topics such as:\n\nIT, CS, SWE and computing in general.\nProgramming.\nSystem operation and administration.\nDevOps!\nDevSecOps.\nDev…SomethingOps?\nSite Reliability Engineering\nSecurity\nmoar software mlemgineering…\nconputer\nWhatever floats my fancy and is vaguely related to the machine colloquially known as the “conputer”.\n"},"kubernetes-theory":{"title":"Kubernetes theory","links":["docker","minikube"],"tags":["k8s","Minikube","containers","orchestrators","term"],"content":"Kubernetes\nFor more low-level practical stuff, go to docs on Docker and K8s basics.\nFor what reason?\nKubernetes can be referred to as the “operating system of the cloud native world”.\nKubernetes connects multiple servers into a cluster, and is thus used to implement a cluster architecture.\nThe “cluster’s brain” is the control plane, which runs all the tasks required for Kubernetes to do its job: scheduling containers, managing services, serving API requests, etc.:\n\nThe control plane is made up of the following main components:\n\nkube-apiserver - frontend server of the control plane.\netcd - adtabase where k8s stores all its config data (nodes, resources, etc).\nkube-scheduler - decides when to run newly created pods.\nkube-controller-manager - running resource controllers such as deployments.\ncloud-controller-manager - interacts with the cloud provider in cloud-based clusters to do stuff.\n\nMaster nodes are members of the cluster which run the control plane components.\nNode components or worker nodes are cluster members that run user workloads.\nEach worker node in a k8s cluster runs these components:\n\nkubelet - responsible for driving the container runtime to start workloads that are scheduled on the node &amp; monitoring their status.\nkube-proxy - doing the networking “magic” between pods, other nodes and the internet.\na container runtime - toy to start and stop containers and handle their communication; just docker or other container software.\n\n\nA correctly configured Kubernetes control plane is supposed to provide high availability by running multiple master nodes. Why? Because if any individual master node were to fail or shut down, or one of the control plane components were to stop abruptly, the cluster shall still work properly.\nA highly available control plane will also handle the situation where the master nodes are working properly, but some of them cannot communicate with the others, due to\na network failure (known as a network partition).\nThe etcd database is replicated across multiple nodes and can survive the failure of individual nodes.\nA good workable minimum of running master nodes on prod is 3.\nA control plane failure would be sorta like the way my OS broke recently1 - your stuff probably still works, you just can’t do nor see shit.\nA worker node failure should not matter, because Kubernetes will detect the failure and reschedule the node’s pods somewhere else, so long as your control plane hasn’t kicked the bucket.\nLarge number of nodes failing synchronously might imply a cluster running out of resources.\n\nA rare, but entirely possible, kind of failure is losing a whole cloud availability zone. Cloud vendors like AWS and Google Cloud provide multiple availability zones in each region, each corresponding roughly to a single data center. For this reason, rather than having all your worker nodes in the same zone, it’s a good idea to distribute them across two or even three zones.\n\nAll in all, the book tells me to trust your stuff, but verify it. I say we should not trust our stuff at all and always double-verify it.\nSimulate disasters, even catastrophes and entire revelations and apocalypses. Reboot your master nodes and try to get them to crash. At least you’ll know what might happen on prod.\nManaged vs. self-hosted - buy or build?\nIf you’re considering self-hosting a biiiig k8s setup, you might want to consider the following questions:\n\n\n                  \n                  k8s self-hosting sanity checklist \n                  \n                \n\n\nIs the control plane highly available, i.e. if a master node goes down or becomes unresponsive, does your cluster still work?\nCan you still deploy or update apps?\nWill your running applications still be fault-tolerant without the\ncontrol plane?\nIs your pool of worker nodes highly available, i.e. if an outage should take down several worker nodes, or even a whole cloud AZ, will your workloads and cluster stop running?\nWill your cluster be able to automatically provision new nodes to self-heal or will manual intervention be required?\nIs your cluster set up securely? Think, TLS, SSL, trusted certificates and not being a dummy that forgets to update them (or refresh your GPG keys2, if relevant).\nDo your users and applications have minimal rights and perms for cluster operations? probably not\nAre container security defaults set properly, i.e. you’re not using defaults that are absolutely deranged to use depending on your use case?\nIs access to the underlying etcd db properly controlled &amp; authenticated?\nAre all your services that are exposed directly to the internet secure? Think HTTPS, SSH, and not being a dummy that lets Jia Tan log into a random Jenkins instance on some orphan IPv4 address.\nIs your cluster conformant to CNCF standards?\nIs your config managed and backed up? Do you use IaC and have a backup backup of it?\nDo you have a backup backup of your cluster?\n\n\n\n\n\n                  \n                  Automated resilience testing tools \n                  \n                \n\nNetflix’s Chaos Monkey.\n\n\nSupplementary book yapping tl;dr:\nTools don’t do it all for you, especially if they promise to be point-and-click, zero-effort or instant gratification (or Gods-forbid “no-code”).\nKubernetes is hard.\nStart off with managed services at first.\nThere are Kubernetes installers that you can use to build your own clusters.\n\n\n                  \n                  k8s cloud-managed madness index \n                  \n                \n\n\nGoogle Kuberetes Service (GKE)\nAmazon Elastic Container Service for Kubernetes (EKS)\n\n\n\nFootnotes\n\n\n15th January 2025 incident involving LightDM and Xorg crashing into oblivion after I flipped the MUX switch; yes, it was a Brazilian BIOS error, also known as “Dumb Animal Operating the System” error. I blame Nvidia drivers, not myself, though. ↩\n\n\nStares menacingly at ASUS Linux &amp; AUR maintainers. ↩\n\n\n"},"linux-kernel":{"title":"Linux kernel","links":[],"tags":["unix","linux","kernel","osdev","EFI","efibootmgr"],"content":"The Linux Kernel\n\n\n                  \n                  docs.kernel.org README \n                  \n                \n\nClick the following link for Linux Docs official README page.\nMost of information presented in this subchapter is a selective copy-paste or paraphrasing from the above website.\n\n\nLinux is a clone of the operating system Unix, written from scratch by Linus Torvalds with assistance from a loosely-knit team of hackers across the Net. It aims towards POSIX and Single UNIX Specification compliance.\nIt has all the features you would expect in a modern fully-fledged Unix, including true multitasking, virtual memory, shared libraries, demand loading, shared copy-on-write executables, proper memory management, and multistack networking including IPv4 and IPv6.\nIt is distributed under the GNU General Public License v2 - see the accompanying COPYING file for more details.\n\n\n                  \n                  Supported hardware \n                  \n                \n\nLinux runs on a variety of hardware architectures, including:\n\n32-bit x86-based PCs (386 or higher)\nMost general-purpose 32- or 64-bit architectures1\nARM\nLinux itself (UML/UserMode Linux)\nCompaq Alpha AXP\nSun SPARC\nUltraSPARC\nMotorola 68000\nPowerPC\nPowerPC64\nHitachi SuperH\nCell\nIBM S/390\nHP PA-RISC\nIntel IA-64\nDEC VAX\nAMD x86-64 Xtensa\nARC\n\n\n\nCompiling the kernel\nRunning the kernel\nBooting a kernel directly from a storage device without the assistance of a bootloader such as LILO or GRUB, is no longer supported in BIOS (non-EFI systems). On UEFI/EFI systems, however, you can use EFISTUB which allows the motherboard to boot directly to the kernel. On modern workstations and desktops, it’s generally recommended to use a bootloader as difficulties can arise with multiple kernels and secure boot. For more details on EFISTUB, see “The EFI Boot Stub” or the Arch Linux Wiki page on EFI boot stub.\nEFI boot stub\nAn EFI boot stub AKA EFI stub is a kernel that is an EFI executable, i.e. that can be directly booted from the UEFI.\nAn example with LTS Linux kernel, NVMe storage and BTRFS filesystem with specific subvolume and hibernation on a swap partition:\nefibootmgr --create \\\n --disk /dev/nvme0n1 --part 1 \\\n --label &quot;EFISTUB Arch&quot; \\\n --loader /vmlinuz-linux-lts \\\n --unicode &#039;root=UUID=01a40dd8-28f0-4636-be1e-aeed60c98095 resume=UUID=2d877d5d-4ca1-4d46-a3d6-b6ee94cbbd78 rw rootflags=subvol=@ loglevel=3 quiet initrd=\\initramfs-linux-lts.img&#039;\nFootnotes\n\n\nProvided they support PMMU (paged memory management unit) and gcc GNU C compiler. ↩\n\n\n"},"ls":{"title":"ls","links":[],"tags":["ls","linux"],"content":"ls\nls is a UNIX command for listing files in a directory.\nAssorted usage examples\nList directories only\nls -d */\n*/ is a pattern that matches all the subdirectories in the current directory."},"minikube":{"title":"Minikube & K8s basics","links":["docker"],"tags":["K8s","Minikube","containers","orchestrators","term"],"content":"Minikube\nA lot of starter stuff here follows from starter Go+Docker stuff at Docker basics.\nDeploying a Go helowoorld on Minikube\nFirst off, ensure you have Docker installed and Docker group set up. It errors in “no docker group” silly-mode and refuses to get sudo’d (good).\nminikube start\nkubectl run demo --image=cloudnatived/demo:hello --port=9999 --labels app=demo\nkubectl port-forward pods/demo 9999:8888\nkubectl get pods --selector app=demo\n\nThe error above also portrays how to deal with some silly little name errors - just read.\nHow to not trainwreck your Minikube\n\n\n                  \n                  Docker network pruning \n                  \n                \n\nDO NOT run:\ndocker network prune\nEspecially if your Minikube cluster isn’t running. It might take down with itself the network necessary for Minikube to start. I am not sure what the simplest, non-destructive (so not involving minikube delete &amp;&amp; minikube start from scratch reset) way to fix this is, but it likely requires you to put together a very unpleasant docker-network command.\nSource: my own mistakes.\n\n"},"nvim":{"title":"NeoVim","links":[],"tags":["neovim","vim","linux"],"content":"NeoVim\nLearning NeoVim from scratch, because one error I don’t get which interfered with my very trivial attempt to navigate the text file pissed me off not to the extreme, but just enough to make me consider nuking and paving my LazyVim “setup”."},"opengraph":{"title":"OpenGraph","links":[],"tags":["web","opengraph","html"],"content":"OpenGraph Protocol\nOpenGraph Protocol or OGP is a web standard for other web applications to create embedded content of your link. The simplest way to add OGP embed to your website, is to add this to its &lt;head&gt; tag:\n&lt;meta property=&quot;og:title&quot; content=&quot;netcatcat.net : subsite&quot;&gt;\n&lt;meta property=&quot;og:type&quot; content=&quot;website&quot;&gt;\n&lt;meta property=&quot;og:url&quot; content=&quot;netcatcat.net/subsite&quot;&gt;\n&lt;meta property=&quot;og:image&quot; content=&quot;netcatcat.net/assets/images/KBITY_TRANSEDEN_TEAL_HEHE_.png&quot;&gt;"},"pgp":{"title":"PGP&GPG","links":[],"tags":["PGP","GPG"],"content":"PGP\n(Pretty Good Privacy)\nPretty good tool for encryption &amp; decryption, as well as verifying file integrity.\nGPG/GnuPG tool\nGnu Privacy Guard - the best free tool (afaik) for doing things with PGP.\nResources:\n\nGnuPG quick command cheatsheet\nEncrypting and decrypting files\n"},"powerstates":{"title":"Device power states","links":[],"tags":["hardware","linux"],"content":"Notes on device power states\nHibernation\nHibernation typically means the entire image of RAM memory is saved to the disk (whether SSD or HDD) and then the laptop is quote-on-quotes shut down.\nSome people claim that hibernation is not the best on Linux-based systems and for SSDs, but it really depends on the drive and distro. A healthy SSD with about 16-32GB or less of memory shouldn’t suffer too much from hibernation. Hibernation issues on Linux are also rare (myself I’ve not run into any)."},"prometheus":{"title":"Prometheus","links":["grafana"],"tags":["sre","monitoring","prometheus"],"content":"Prometheus\nPrometheus is a tool for collecting data for monitoring purposes. Basically it’s one of the “backend options” for monitoring visualization tools such as Grafana.\nTime-series data\nA concept fundamental to data collected by Prometheus. It’s as if there was a collection of same-type ordered pairs of some metric m and measured time t; “time measured” typically refers to either absolute time or time recorded in relation to Prometheus or its data source being started."},"raspberrypi":{"title":"My Raspberry Pi Zero W","links":[],"tags":["hardware","embedded","rpi","linux"],"content":"Raspberry Pi Zero W\nMy little toy\n2025-01-30: I have assembled it; the heatsink is attached and the microcomputer is inserted into the cover.\nI put the 16 GB microSD card into the adapter in order to install the OS. Will have to do that via MSI, as ASUS hadn’t built into their caca laptop a port for it."},"robots.txt":{"title":"robots.txt","links":[],"tags":["robots.txt","sitemap.xml","web-scraping","web-crawling"],"content":"robots.txt\nA semi-informal standard made by Google to tell webscraping robots (provided they are polite; some aren’t) to fuck off.\nSee official Google documentation here.\nAll robots.txt files should be encoded in UTF-8.\nExample robots.txt file:\nUser-agent: Googlebot\nDisallow: /nogooglebot/\n \nUser-agent: *\nAllow: /\n \nSitemap: www.example.com/sitemap.xml\nsitemap.xml"},"rust":{"title":"Rust","links":[],"tags":["rust"],"content":"Rust"},"sre-principles":{"title":"SRE principles","links":["prometheus"],"tags":["sre","monitoring"],"content":"SRE principles\nMonitoring\nConcepts from João Pereira’s 52 weeks of SRE (W2)\nBlack-box and white-box\nA name familiar from testing concepts; we can also categorize monitoring into these two:\n\n\n                  \n                  White-box monitoring \n                  \n                \n\nExamines system internals and detailed metrics.\nProvides insights into system behaviour and performance.\n\n\n                  \n                  Example\n                  \n                \n\n\nMemory usage and garbage collection metrics.\nDatabase query performance.\nInternal queue lenghts (e.g. monitoring a Kafka consumer’s offset lag).\nCache hit rates.\nApplication-specific metrics.\n\n\n\n\n\n\n\n                  \n                  Black-box monitoring \n                  \n                \n\n\nTest system behaviour from the outside, like how an end user would experience it.\nFocuses on system outputs and external behaviours.\n\n\n\n                  \n                  Examples \n                  \n                \n\n\nHTTP endpoint availability checks (can I reach this endpoint?).\nAPI response time measurements (is this request taking too long?).\nEnd-to-end transaction tests (did this operation fulfill all of its requirements?).\n\n\n\n\n\nAlerts\nAn alert’s rule defines the conditions that trigger the alert. Typically an alert also has severity attached to it.\nOne of the best practices for setting up alerts is ensuring that all alerts are actionable:\n\nEvery alert should require human intervention and have a clear resolution path. If an alert is triggered but there’s nothing an engineer can do to resolve it, then it shouldn’t be an alert.\n\nMake sure rules are not too permissive nor aggressive to avoid alarm fatigue.\nAnother one is alerting symptoms rather than causes, e.g.:\n\n\n                  \n                  Recommended examples \n                  \n                \n\n\nAPI success rate below 99.9%\nPayment processing latency &gt; 10s\nError rate exceeded 5% in the last 5 mins\n\n\n\n\n\n                  \n                  Not recommended examples \n                  \n                \n\n\nHigh CPU usage on an API server\nNetwork packets received have increased\nLow disk space on server\n\n\n\nThe handling of situations from not recommended examples can often be automated with auto-scaling or failover mechanisms (if truly necessary).\nDashboard strategies\n\n\n                  \n                  USE method \n                  \n                \n\nFor monitoring infrastructure; best used for building dashboards that track hardware resources in infrastructure, such as CPU, memory, and network devices. It tells you how happy your machines are.\n\nUtilization: Percentage time the resource is busy (e.g.: CPU usage).\nSaturation: Amount of work a resource has to do (e.g.: queue length).\nErrors: Count of error events.\n\n\n\n\n\n                  \n                  RED method \n                  \n                \n\nFor monitoring user experience; most applicable to building dashboards for services, especially a microservices environment. A well-designed RED dashboard is a proxy for user experience, telling you how happy your customers are.\n\nRate: Requests per second\nErrors: Number of failing requests\nDuration: Request latency distribution\n\n\n\n\n\n                  \n                  The Four Golden Signals \n                  \n                \n\nAccording to the Google SRE handbook, if you can only measure four metrics of your user-facing system, focus on these four.\n\nLatency: Time to serve requests\nTraffic: System demand\nErrors: Failed request rate\nSaturation: System fullness\n\n\n\nTypes of dashboards, in short:\n\nOverview dashboards (general system health checks, rapid anomaly detection).\nService-specific dashboards (detailed data for specific services and their internals).\nBusiness metric dashboards (for non-technical stakeholders and business-relevant data).\n\nDashboard best practices\n\nLine-graphs (time-series data)\n\nIdeal for latency, request volumes &amp; error rates\nShows variations over time\nTrend analysis\n\n\nGauges (utilization metircs)\n\nIdeal for CPU, memory, Service Level Objectives\nShows current values within defined ranges\nQuick visual reference for usage and limits\n\n\nTables (detailed breakdowns)\n\nIdeal for complex reports &amp; database queries\nMultiple dataset visualization\nDetailed analytical view\n\n\nHeat-maps (distribution)\n\nIdeal for pattern identification over time\nValue distribution visualization\nTemporal pattern analysis\n\n\n"},"ssh":{"title":"SSH","links":[],"tags":["ssh","linux","term"],"content":"OpenSSH (Secure Shell)\nOpenSSH is a collection of tools, here’s a brief overview of them all:\n\nssh - “Secure Shell”, the command that allows a secure tunnel for a shell connection to some other device.\nsshd - The OpenSSH daemon, which allows SSH connections to be made to the device running it.\n\nssh\nsshd\nNot installed by default on Linux Mint.\nsudo apt install openssh-server\n/etc/ssh/sshd_config\nThe sshd server system-wide configuration file. It has a manpage."},"unix-permissions":{"title":"Unix permissions","links":[],"tags":["unix","linux","permissions","chmod","term"],"content":"Unix permissions with chmod\n\nUseful examples:\nMake it executable:\nchmod +x ./run.sh\nRoot only file access:\nchmod 000 ~/.creds\nRead-only exclusively for file owner:\nchmod 400 ./special-toy\nFull permissions for everyone (what are you even doing):1\nchmod 777 /my-stupid-root-folder\nFootnotes\n\n\nBeen there, done that. ↩\n\n\n"},"vim-cheatsheet":{"title":"Vim cheatsheet","links":[],"tags":["vim","neovim","term"],"content":"Vi/Vim/Neovim(/Nano) Cheatsheet\nEmergency FAQ zone:\nQ :: “Oh man aw mane, we  are in som troble”\nA :: &lt;ESC&gt;+qa!+&lt;ENTER&gt;\nQ :: “Aaaa, why cannot I autocomplete file selections in my nooby Neovim setup!?”\nA :: &lt;CTRL&gt;+&lt;y&gt;\nQ :: “What!! WHate !! WHAET!!! How do I . SUBSTITUTE !!! GLOBALLY !!!1111”\nA :: &lt;ESC&gt;+:%s/ReplaceMe/WithThis/+&lt;ENTER&gt;\nGeneral cheatsheet (Vi family)\nGlobal substitution :: &lt;ESC&gt;+:%s/findand/replacewith1\nYank/etc into clipboard :: &lt;SHIFT&gt;+&lt;&#039;&gt;+&lt;=&gt;+&lt;y&gt; / &lt;&quot;&gt;+&lt;+&gt;+&lt;y&gt;\nPaste/etc from clipboard :: &lt;SHIFT&gt;+&lt;&#039;&gt;+&lt;=&gt;+&lt;p&gt; / &lt;&quot;&gt;+&lt;+&gt;+&lt;p&gt;\nDelete inside two HTML/XML tags :: dit, &lt;d&gt;+&lt;i&gt;+&lt;t&gt;\ne.g.: &lt;p id=&quot;catnet&quot;&gt;purge me&lt;/p&gt; → &lt;p id=&quot;catnet&quot;&gt;&lt;/p&gt;\nGo to first line :: &lt;g&gt;+&lt;g&gt;\nGo to beginning of current line :: &lt;0&gt;\nGo to beginning of current line :: &lt;$&gt; / &lt;SHIFT&gt;+&lt;4&gt;\nAppend arbitrary identical characters in an arbitrary vertical line:\n&lt;CTRL&gt;+&lt;v&gt; → &lt;j&gt; or &lt;k&gt; as much as needed → &lt;I&gt; / &lt;SHIFT&gt;+&lt;i&gt; → type your arbitrary character collection! → &lt;ESC&gt;\nCount matches and display result :: :%s/pattern//gn+&lt;ENTER&gt;\n\n\n                  \n                  Example \n                  \n                \n\nBefore edit:\nThe key sequence:\n \n`0&lt;Ctrl&gt;v1jI&gt; &lt;Esc&gt;`\nThe key sequence:\n0&lt;Ctrl&gt;v2jI&gt; &lt;Esc&gt;\nAfter edit:\n&gt; The key sequence:\n&gt; \n&gt; `0&lt;Ctrl&gt;v1jI&gt; &lt;Esc&gt;`\n\n\nFootnotes\n\n\nAppend /g if global substitution doesn’t work. ↩\n\n\n"}}